% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/odbc-connection.R, R/driver-spark.R
\name{odbcConnectionSchemas}
\alias{odbcConnectionSchemas}
\alias{odbcConnectionSchemas,OdbcConnection-method}
\alias{odbcConnectionSchemas,Spark SQL-method}
\title{odbcConnectionSchemas}
\usage{
odbcConnectionSchemas(conn, catalog_name)

\S4method{odbcConnectionSchemas}{OdbcConnection}(conn, catalog_name = NULL)
}
\arguments{
\item{conn}{OdbcConnection}

\item{catalog_name}{Catalog where
we are looking to list schemas.}
}
\description{
This function returns a listing of available
schemas.
}
\details{
Currently, for a generic connection the
catalog_name argument is ignored.

Databricks supports multiple catalogs.  On the other hand,
the default implementation of \code{odbcConnectionSchemas} which routes through
\code{SQLTables} is likely to enumerate the schemas in the currently active
catalog only.

This implementation will respect the \code{catalog_name} arrgument.
}
\keyword{internal}
